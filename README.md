# â—ˆ ASTRA â€” Personal AI OS

> Built by Arnav Yadav Â· Runs 100% locally on Mac M4 Â· No cloud, no API keys

ASTRA is a multimodal personal AI assistant with chat, voice, vision, and memory â€” all running locally via Ollama.

---

## ğŸš€ What It Can Do

| Feature | Status | Details |
|---|---|---|
| ğŸ’¬ Chat | âœ… | Multi-model routing, intent detection, web search |
| ğŸ¤ Voice | âœ… | STT (Whisper), TTS (macOS Samantha), wake word |
| ğŸ‘ï¸ Vision | âœ… | WebRTC camera stream, screen capture, LLaVA analysis |
| ğŸ—£ï¸ Talk | âœ… | Voice + vision combined â€” speak while showing camera |
| ğŸ§  Memory | âœ… | Facts, emotions, summaries, ChromaDB vector search |
| ğŸŒ Web Search | âœ… | DuckDuckGo search agent with citations |
| ğŸ¤– Multi-Agent | âœ… | Reasoner, critic, refinement pipeline |
| ğŸ“ File Reader | âœ… | Read and analyze local files |
| ğŸ’» System Monitor | âœ… | CPU, RAM, disk, top processes |
| ğŸ Python Sandbox | âœ… | Propose and execute Python code |
| ğŸ”€ Git Tools | âœ… | Status, log, branch, diff, commit |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Frontend                    â”‚
â”‚         React + Vite (localhost:3000)        â”‚
â”‚   â—ˆ CHAT tab          â— VISION tab          â”‚
â”‚   Voice controls      WebRTC camera          â”‚
â”‚   Memory panel        Talk to ASTRA          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ HTTP / REST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Flask Backend                  â”‚
â”‚            (localhost:5050)                  â”‚
â”‚                                              â”‚
â”‚  /chat        â†’ Brain pipeline               â”‚
â”‚  /talk        â†’ Voice + Vision combined      â”‚
â”‚  /vision/*    â†’ LLaVA analysis               â”‚
â”‚  /voice/*     â†’ STT / TTS / wake word        â”‚
â”‚  /memory      â†’ Load / save / clear          â”‚
â”‚  /model/*     â†’ Switch Ollama model          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Ollama                       â”‚
â”‚   phi3:mini   mistral   llama3.2   llava:7b  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Backend Modules

```
backend/
â”œâ”€â”€ api/           â†’ Flask blueprints (chat, voice, vision, multimodal)
â”œâ”€â”€ core/          â†’ Brain, model manager, confidence, truth guard
â”œâ”€â”€ agents/        â†’ Reasoner, critic, knowledge agent
â”œâ”€â”€ memory/        â†’ Engine, extractor, semantic recall, ChromaDB
â”œâ”€â”€ emotion/       â†’ Detector, memory, responder
â”œâ”€â”€ vision/        â†’ LLaVA analyzer, capture, vision engine
â”œâ”€â”€ voice/         â†’ Whisper listener, TTS speaker, wake word
â”œâ”€â”€ intents/       â†’ Shortcuts, classifier, mood
â”œâ”€â”€ tools/         â†’ File reader, git, system monitor, python sandbox
â”œâ”€â”€ websearch/     â†’ DuckDuckGo search agent
â”œâ”€â”€ reflection/    â†’ Reply refinement
â””â”€â”€ utils/         â†’ Cleaner, polisher, limiter, language detector
```

---

## âš¡ Quick Start

### Requirements

- Mac M4 (or any Mac with Apple Silicon)
- Python 3.11+
- Node.js 18+
- [Ollama](https://ollama.ai)

### 1. Clone

```bash
git clone https://github.com/TDevViper/Astra_Presonal_ai.git
cd Astra_Presonal_ai
```

### 2. Pull Models

```bash
ollama pull phi3:mini
ollama pull mistral
ollama pull llama3.2:3b
ollama pull llava:7b
```

### 3. Backend

```bash
cd backend
python -m venv venv311
source venv311/bin/activate
pip install -r requirements.txt
python app.py
```

Backend starts at `http://localhost:5050`

### 4. Frontend

```bash
cd frontend
npm install
npm run dev
```

Frontend at `http://localhost:3000`

---

## ğŸ¤ Voice Setup

Voice uses macOS built-in `say` command (no setup needed) + faster-whisper for STT.

```bash
pip install faster-whisper sounddevice pyttsx3 numpy
```

Test voice:
```bash
curl -X POST http://localhost:5050/voice/say \
  -H "Content-Type: application/json" \
  -d '{"text": "ASTRA online"}'
```

---

## ğŸ‘ï¸ Vision Setup

Vision uses your browser's WebRTC camera + LLaVA running in Ollama.

```bash
ollama pull llava:7b
```

In the UI â€” switch to `â— VISION` tab â†’ click **START CAMERA** â†’ allow browser camera permission.

**Vision capabilities:**
- ğŸ“· Camera analysis â€” ASTRA describes what it sees
- ğŸ–¥ï¸ Screen capture â€” analyze your IDE, terminal, browser
- ğŸ¤ Talk mode â€” speak + show camera, ASTRA responds out loud
- â–¶ï¸ Live mode â€” continuous analysis every 8 seconds

---

## ğŸ§  Memory System

ASTRA remembers across sessions using:

- **JSON store** â€” preferences, facts, emotions, summaries
- **ChromaDB** â€” vector embeddings for semantic search
- **Sentence Transformers** â€” `all-MiniLM-L6-v2` embeddings

Memory is stored in `backend/memory/data/memory.json`

Clear memory:
```bash
curl -X DELETE http://localhost:5050/memory
```

---

## ğŸ¤– Multi-Model Routing

ASTRA automatically selects the best model per query:

| Query Type | Model |
|---|---|
| Casual chat, greetings | phi3:mini |
| Technical / code | mistral |
| Research / analysis | llama3.2:3b |
| Vision | llava:7b |

Switch model via UI or:
```bash
curl -X POST http://localhost:5050/model/switch \
  -H "Content-Type: application/json" \
  -d '{"model": "mistral:latest"}'
```

---

## ğŸŒ API Reference

```
POST /chat              â†’ Send message, get reply
POST /talk              â†’ Voice + vision combined
POST /talk/listen       â†’ Record voice â†’ transcribe
POST /voice/say         â†’ TTS speak text
POST /voice/listen      â†’ Record + transcribe
POST /voice/start       â†’ Start wake word mode
POST /vision/screen     â†’ Analyze screen
POST /vision/camera     â†’ Analyze camera (OpenCV)
POST /vision/analyze_b64 â†’ Analyze base64 image (WebRTC)
GET  /vision/last/<src> â†’ Get last snapshot
GET  /memory            â†’ Load memory
DELETE /memory          â†’ Clear memory
POST /model/switch      â†’ Switch Ollama model
GET  /health            â†’ Health check
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Tech |
|---|---|
| Frontend | React, Vite, WebRTC |
| Backend | Python, Flask, Flask-CORS |
| LLM | Ollama (phi3, mistral, llama3.2, llava) |
| Vision | LLaVA:7b, OpenCV, mss |
| STT | faster-whisper (tiny model) |
| TTS | macOS `say`, pyttsx3 |
| Memory | ChromaDB, sentence-transformers |
| Web Search | DuckDuckGo |

---

## ğŸ“ Project Structure

```
Astra/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ vision/
â”‚   â”œâ”€â”€ voice/
â”‚   â”œâ”€â”€ memory/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ LiveVision.jsx
â”‚   â”‚       â””â”€â”€ ChatMessage.jsx
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md
```

---

## ğŸ”® Roadmap

- [ ] WebSocket real-time streaming responses
- [ ] Temporal frame memory (last 10 frames context)
- [ ] Object tracking across frames
- [ ] Emotion detection from facial expressions
- [ ] Self-improvement loop â€” log and learn from sessions
- [ ] Android / iOS companion app
- [ ] 3060 server offloading for heavy models

---

*Built with ğŸ”¥ by Arnav Yadav*